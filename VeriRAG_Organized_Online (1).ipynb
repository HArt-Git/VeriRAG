{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bbc6835",
   "metadata": {},
   "source": [
    "# VeriRAG: Knowledge Graph-Augmented RAG for Verilog and Assertions\n",
    "\n",
    "**What this implements.**\n",
    "- **Ingestion → Parsing → RDF Knowledge Graph (KG) → Vector Store → Retrieval → Prompting → Generation.**\n",
    "- Two generation paths: **RTL** (large synthesizable Verilog) and **SVA** (Verilog with immediate assertions).\n",
    "- **Interactive loops** so you can issue multiple queries in one session.\n",
    "\n",
    "**High-level flow.**\n",
    "1. Upload a CSV of training examples (instructions + Verilog code, or raw modules).\n",
    "2. Parse Verilog to extract **ports, signals, parameters, and operations** with Pyverilog + heuristics.\n",
    "3. Turn the parsed structure into an **RDF KG** (Turtle format) using `rdflib`.\n",
    "4. Summarize each module with the LLM and compute **OpenAI embeddings**.\n",
    "5. Store text + embeddings + metadata in **ChromaDB**.\n",
    "6. At query time, use **vector retrieval** + **KG-derived metadata** to build **LLM prompts** for RTL or SVA code generation.\n",
    "\n",
    "**Prerequisites.**\n",
    "- This notebook is designed for Google Colab environments (mounts Google Drive under `/content/drive`).\n",
    "- You must provide a valid **OpenAI API key**.\n",
    "- You should have a CSV file compatible with the expected schema (see Section 7 for details).\n",
    "\n",
    "**Outputs.**\n",
    "- Per-chunk Turtle files in `.../knowledge_graphs/` (RDF representation of modules).\n",
    "- A persistently stored ChromaDB collection (vector index + metadata).\n",
    "- LLM-generated Verilog modules (printed in the cell output for inspection/copying).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd9364",
   "metadata": {},
   "source": [
    "## 1. Imports & Logging\n",
    "These imports match the original scripts. They enable:\n",
    "- **RDF/KG** operations (`rdflib`)\n",
    "- **Vector DB** persistence (`chromadb`)\n",
    "- **OpenAI** chat + embedding clients (`openai`)\n",
    "- **Verilog parsing** (`pyverilog`)\n",
    "- **I/O and utilities** (CSV, JSON, regex, logging)\n",
    "\n",
    "Logging is configured once to surface key status messages and parsing diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94401734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, Literal, RDF, RDFS, Namespace, URIRef\n",
    "import chromadb\n",
    "import openai\n",
    "import logging\n",
    "import json\n",
    "import urllib.parse\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import uuid\n",
    "import io\n",
    "import tiktoken\n",
    "from contextlib import redirect_stderr\n",
    "from google.colab import files, drive\n",
    "import pyverilog.vparser.parser as vparser\n",
    "from pyverilog.vparser.parser import parse, Description, ModuleDef, Ioport, Port\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c40e0",
   "metadata": {},
   "source": [
    "## 2. OpenAI API\n",
    "**Why:** Credentials should not be hard-coded. This cell obtains the key from `OPENAI_API_KEY` or prompts securely.\n",
    "\n",
    "**Used by:** summarization (module overviews), embeddings (vector search), and both generation paths (RTL/SVA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        OPENAI_API_KEY = getpass.getpass('Enter OPENAI_API_KEY (input hidden): ')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('OPENAI_API_KEY is required for online-only mode.')\n",
    "\n",
    "openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "print('OpenAI client initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d96492c",
   "metadata": {},
   "source": [
    "## 3. Storage: Google Drive Mount & Directories\n",
    "**What it does:**\n",
    "- Mounts Google Drive at `/content/drive` (typical for Colab) so intermediate files persist across sessions.\n",
    "- Creates the working directories:\n",
    "  - `PERSIST_DIRECTORY`: top-level workspace for this project.\n",
    "  - `LOCAL_DB_DIR`: scratch area (kept for parity with original code).\n",
    "  - `knowledge_graphs/`: per-chunk Turtle files (RDF graphs).\n",
    "\n",
    "**Note:** paths mirror the original scripts so any existing artifacts continue to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "drive.mount('/content/drive')\n",
    "PERSIST_DIRECTORY = '/content/drive/MyDrive/GRAPHRAG5'\n",
    "LOCAL_DB_DIR = '/content/rag_db'\n",
    "os.makedirs(PERSIST_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(LOCAL_DB_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(PERSIST_DIRECTORY, 'knowledge_graphs'), exist_ok=True)\n",
    "print('Dirs ready:', PERSIST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e1771",
   "metadata": {},
   "source": [
    "## 4. Summarization Function (Module Overviews)\n",
    "**Purpose:** produce concise, 100–200 word summaries of each Verilog module for retrieval cues.\n",
    "\n",
    "**Inputs:**\n",
    "- `code`: raw Verilog text for a single module.\n",
    "- `instruction`: the instruction string paired with the module (if present in the CSV).\n",
    "\n",
    "**Output:** string summary (logged + returned). If the LLM call fails, the function returns a fallback error string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_code(code, instruction):\n",
    "    prompt = f\"\"\"\n",
    "    Provide a detailed summary of the following Verilog module, including its functionality, inputs, outputs, parameters, and key operations. The instruction provided for the module is: \"{instruction}\".\n",
    "    Verilog Code:\n",
    "    ```verilog\n",
    "    {code}\n",
    "    ```\n",
    "    Summary should be concise (100-200 words) and focus on:\n",
    "    - Purpose of the module\n",
    "    - Inputs and outputs (including widths)\n",
    "    - Parameters (if any)\n",
    "    - Main operations or logic\n",
    "    - Any notable features (e.g., sequential, combinational, FSM)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model='gpt-4',\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        print(f'Generated summary: {summary[:100]}...')\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error generating summary: {str(e)}')\n",
    "        return 'Summary generation failed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49aa664",
   "metadata": {},
   "source": [
    "## 5. Verilog Parsing & Operation Classification\n",
    "**Parsing strategy:**\n",
    "1. Use **Pyverilog** when possible to build an AST and extract ports.\n",
    "2. Fall back to **regex-based heuristics** to collect ports, signals, parameters, instantiations, and assignments.\n",
    "\n",
    "**Operation tagging:** `classify_operation()` assigns coarse labels (e.g., `AND`, `OR`, `ADD`, `ASSIGN`, etc.) to expressions for KG edges.\n",
    "\n",
    "**Outputs:**\n",
    "- `module_name`\n",
    "- `input_ports`, `output_ports`, `signals`, `parameters`\n",
    "- `operations` with IDs, types, operands, and context (combinational/sequential)\n",
    "- `ast` (when Pyverilog succeeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c08145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_verilog(code):\n",
    "    def replace_idx(match):\n",
    "        idx = match.group(1)\n",
    "        return f'IDX{idx}'\n",
    "    code = re.sub(r'`IDX\\((\\d+)\\)', replace_idx, code)\n",
    "    return code\n",
    "\n",
    "def classify_operation(expr):\n",
    "    expr = expr.strip()\n",
    "    if '&' in expr and not '&&' in expr:\n",
    "        op_type = 'AND'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr.replace('&', ' '))\n",
    "    elif '|' in expr and not '||' in expr:\n",
    "        op_type = 'OR'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr.replace('|', ' '))\n",
    "    elif '^' in expr:\n",
    "        op_type = 'XOR'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr.replace('^', ' '))\n",
    "    elif '+' in expr:\n",
    "        op_type = 'ADD'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr.replace('+', ' '))\n",
    "    elif '-' in expr and not '->' in expr:\n",
    "        op_type = 'SUBTRACT'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr.replace('-', ' '))\n",
    "    elif '<<' in expr:\n",
    "        op_type = 'LSHIFT'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr.replace('<<', ' '))\n",
    "    elif '>>' in expr:\n",
    "        op_type = 'RSHIFT'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr.replace('>>', ' '))\n",
    "    elif '~' in expr:\n",
    "        op_type = 'NOT'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr.replace('~', ' '))\n",
    "    elif '<=' in expr:\n",
    "        op_type = 'NON_BLOCKING_ASSIGN'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr)\n",
    "    elif '=' in expr and '==' not in expr:\n",
    "        op_type = 'ASSIGN'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr)\n",
    "    else:\n",
    "        op_type = 'UNKNOWN'\n",
    "        operands = re.findall(r'\\b[\\w\\[\\]:]+\\b', expr)\n",
    "    return op_type, operands\n",
    "\n",
    "def parse_verilog_code(code, temp_file='temp.v'):\n",
    "    module_name = None\n",
    "    input_ports = []\n",
    "    output_ports = []\n",
    "    signals = []\n",
    "    parameters = []\n",
    "    operations = []\n",
    "    ast = None\n",
    "    header_ports = []\n",
    "    port_directions = {}\n",
    "\n",
    "    code = preprocess_verilog(code)\n",
    "    with open(temp_file, 'w') as f:\n",
    "        f.write(code)\n",
    "\n",
    "    try:\n",
    "        f = io.StringIO()\n",
    "        with redirect_stderr(f):\n",
    "            ast, _ = parse([temp_file], preprocess_include=['/content/verilog/'], debug=False)\n",
    "        if isinstance(ast.description, Description):\n",
    "            for node in ast.description.definitions:\n",
    "                if isinstance(node, ModuleDef):\n",
    "                    module_name = node.name\n",
    "                    if node.portlist:\n",
    "                        for port in node.portlist.ports:\n",
    "                            if isinstance(port, Ioport) and hasattr(port.first, 'name'):\n",
    "                                port_name = port.first.name\n",
    "                                width = '1'\n",
    "                                if hasattr(port.first, 'width') and port.first.width:\n",
    "                                    width = f'[{port.first.width.msb}:{port.first.width.lsb}]'\n",
    "                                if isinstance(port.first, vparser.Input):\n",
    "                                    input_ports.append((port_name, width))\n",
    "                                elif isinstance(port.first, vparser.Output):\n",
    "                                    output_ports.append((port_name, width))\n",
    "                            elif isinstance(port, Port) and hasattr(port, 'name'):\n",
    "                                header_ports.append((port.name, '1'))\n",
    "    except Exception as e:\n",
    "        print(f'Pyverilog parsing failed: {str(e)}')\n",
    "\n",
    "    try:\n",
    "        lines = code.splitlines()\n",
    "        module_found = False\n",
    "        in_module_decl = False\n",
    "        port_section = []\n",
    "        i = 0\n",
    "        always_context = None\n",
    "\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.startswith('//') or not line:\n",
    "                i += 1\n",
    "                continue\n",
    "            if line.startswith('module'):\n",
    "                candidate_name = line.split()[1].split('(')[0].strip()\n",
    "                module_name = candidate_name\n",
    "                module_found = True\n",
    "                if '(' in line:\n",
    "                    in_module_decl = True\n",
    "                    start_idx = line.index('(') + 1\n",
    "                    if ')' in line:\n",
    "                        port_section.append(line[start_idx:line.index(')')])\n",
    "                        in_module_decl = False\n",
    "                    else:\n",
    "                        port_section.append(line[start_idx:])\n",
    "                i += 1\n",
    "                continue\n",
    "            if in_module_decl:\n",
    "                if ')' in line:\n",
    "                    port_section.append(line[:line.index(')')])\n",
    "                    in_module_decl = False\n",
    "                else:\n",
    "                    port_section.append(line)\n",
    "                i += 1\n",
    "                continue\n",
    "            if module_found:\n",
    "                port_match = re.match(r'^(input|output|inout)\\s*(wire|reg)?\\s*(\\[[\\w\\-`]+:[0-9]+\\])?\\s*([\\w,\\s]+)\\s*[,;]', line)\n",
    "                if port_match:\n",
    "                    direction = port_match.group(1)\n",
    "                    width = port_match.group(3) if port_match.group(3) else '1'\n",
    "                    port_names = [p.strip() for p in port_match.group(4).split(',') if p.strip()]\n",
    "                    for port_name in port_names:\n",
    "                        port_directions[port_name] = direction\n",
    "                        if direction == 'input' and port_name not in [p[0] for p in input_ports]:\n",
    "                            input_ports.append((port_name, width))\n",
    "                        elif direction == 'output' and port_name not in [p[0] for p in output_ports]:\n",
    "                            output_ports.append((port_name, width))\n",
    "                signal_match = re.match(r'^(wire|reg)\\s*(\\[[\\w\\-`]+:[0-9]+\\])?\\s*([\\w,\\s]+)\\s*;', line)\n",
    "                if signal_match:\n",
    "                    signal_type = signal_match.group(1)\n",
    "                    width = signal_match.group(2) if signal_match.group(2) else '1'\n",
    "                    signal_names = [s.strip() for s in signal_match.group(3).split(',')]\n",
    "                    for signal_name in signal_names:\n",
    "                        if signal_name not in [p[0] for p in input_ports + output_ports]:\n",
    "                            signals.append((signal_name, signal_type, width))\n",
    "                param_match = re.match(r'^parameter\\s+(.+?);', line)\n",
    "                if param_match:\n",
    "                    param_str = param_match.group(1).strip()\n",
    "                    param_pairs = re.split(r',\\s*(?=\\w+\\s*=)', param_str)\n",
    "                    for pair in param_pairs:\n",
    "                        pair_match = re.match(r'(\\w+)\\s*=\\s*([^,\\s]+(?:\\s*[^,\\s]+)*)', pair.strip())\n",
    "                        if pair_match:\n",
    "                            param_name = pair_match.group(1).strip()\n",
    "                            param_value = pair_match.group(2).strip()\n",
    "                            print(f'Parsed parameter: name={param_name}, value={param_value}')\n",
    "                            parameters.append((param_name, param_value))\n",
    "                inst_match = re.match(r'^(\\w+)\\s+(\\w+)\\s*\\(([^)]+)\\);', line)\n",
    "                if inst_match:\n",
    "                    module_type = inst_match.group(1)\n",
    "                    instance_name = inst_match.group(2)\n",
    "                    ports = [p.strip() for p in inst_match.group(3).split(',')]\n",
    "                    operations.append({\n",
    "                        'id': str(uuid.uuid4()),\n",
    "                        'type': 'INSTANTIATION',\n",
    "                        'target': instance_name,\n",
    "                        'expression': f\"{module_type}({', '.join(ports)})\",\n",
    "                        'operands': ports,\n",
    "                        'context': 'structural'\n",
    "                    })\n",
    "                assign_match = re.match(r'^assign\\s+([\\w\\[\\]:]+)\\s*=\\s*([^;]+);', line)\n",
    "                if assign_match:\n",
    "                    target = assign_match.group(1)\n",
    "                    expr = assign_match.group(2).strip()\n",
    "                    op_type, operands = classify_operation(expr)\n",
    "                    operations.append({\n",
    "                        'id': str(uuid.uuid4()),\n",
    "                        'type': op_type,\n",
    "                        'target': target,\n",
    "                        'expression': expr,\n",
    "                        'operands': operands,\n",
    "                        'context': 'combinational'\n",
    "                    })\n",
    "                if line.startswith('always @'):\n",
    "                    if '@(*)' in line or '@(' in line and 'posedge' not in line:\n",
    "                        always_context = 'combinational'\n",
    "                    elif 'posedge' in line:\n",
    "                        always_context = 'sequential'\n",
    "                    i += 1\n",
    "                    while i < len(lines) and not lines[i].strip().startswith('endmodule'):\n",
    "                        stmt = lines[i].strip()\n",
    "                        if stmt and not stmt.startswith('//'):\n",
    "                            nb_assign_match = re.match(r'^([\\w\\[\\]:]+)\\s*<\\=\\s*([^;]+);', stmt)\n",
    "                            if nb_assign_match:\n",
    "                                target = nb_assign_match.group(1).strip()\n",
    "                                expr = nb_assign_match.group(2).strip()\n",
    "                                op_type, operands = classify_operation(expr)\n",
    "                                operations.append({\n",
    "                                    'id': str(uuid.uuid4()),\n",
    "                                    'type': op_type,\n",
    "                                    'target': target,\n",
    "                                    'expression': expr,\n",
    "                                    'operands': operands,\n",
    "                                    'context': always_context\n",
    "                                })\n",
    "                            block_assign_match = re.match(r'^([\\w\\[\\]:]+)\\s*=\\s*([^;]+);', stmt)\n",
    "                            if block_assign_match:\n",
    "                                target = block_assign_match.group(1).strip()\n",
    "                                expr = block_assign_match.group(2).strip()\n",
    "                                op_type, operands = classify_operation(expr)\n",
    "                                operations.append({\n",
    "                                    'id': str(uuid.uuid4()),\n",
    "                                    'type': op_type,\n",
    "                                    'target': target,\n",
    "                                    'expression': expr,\n",
    "                                    'operands': operands,\n",
    "                                    'context': always_context\n",
    "                                })\n",
    "                        i += 1\n",
    "                    continue\n",
    "            i += 1\n",
    "\n",
    "        if port_section:\n",
    "            port_text = ' '.join(port_section).replace(';', ',')\n",
    "            port_list = [p.strip() for p in port_text.split(',') if p.strip() and not p.strip().startswith('//')]\n",
    "            for port in port_list:\n",
    "                match = re.match(r'^(input|output|inout)?\\s*(wire|reg)?\\s*(\\[[\\w\\-`]+:[0-9]+\\])?\\s*(\\w+)', port)\n",
    "                if match and match.group(1):\n",
    "                    width = match.group(3) if match.group(3) else '1'\n",
    "                    port_name = match.group(4)\n",
    "                    direction = match.group(1)\n",
    "                    port_directions[port_name] = direction\n",
    "                    if direction == 'input' and port_name not in [p[0] for p in input_ports]:\n",
    "                        input_ports.append((port_name, width))\n",
    "                    elif direction == 'output' and port_name not in [p[0] for p in output_ports]:\n",
    "                        output_ports.append((port_name, width))\n",
    "                else:\n",
    "                    header_ports.append((port, '1'))\n",
    "\n",
    "        for port_name, width in header_ports:\n",
    "            direction = port_directions.get(port_name, 'input')\n",
    "            if direction == 'input' and port_name not in [p[0] for p in input_ports]:\n",
    "                input_ports.append((port_name, width))\n",
    "            elif direction == 'output' and port_name not in [p[0] for p in output_ports]:\n",
    "                output_ports.append((port_name, width))\n",
    "\n",
    "        if not module_found:\n",
    "            print('No valid module found in code')\n",
    "        else:\n",
    "            print(f'Parsed module: {module_name}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Heuristic parsing failed: {str(e)}')\n",
    "\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "\n",
    "    return module_name, input_ports, output_ports, signals, parameters, operations, ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85144a2c",
   "metadata": {},
   "source": [
    "## 6. Entity Extraction & RDF Knowledge Graph (RDF/Turtle)\n",
    "**Why RDF:** graph structure lets us express relationships between **modules, parameters, signals, and operations**.\n",
    "\n",
    "**Entities:** `Module`, `Signal`, `Parameter`, `Operation`.\n",
    "**Relations:** `hasInput`, `hasOutput`, `hasInternalSignal`, `hasParameter`, `performsOperation`, `usesSignal`, `producesSignal`, `dependsOnParameter`, `usesParameter`, `instantiates`.\n",
    "\n",
    "**Output:** a `.ttl` file per chunk in `knowledge_graphs/`, which can be inspected with standard RDF tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95344a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(module_name, input_ports, output_ports, signals, parameters, operations, ast=None):\n",
    "    modules = [{\n",
    "        'name': module_name,\n",
    "        'input_ports': [{'name': name, 'direction': 'input', 'width': width} for name, width in input_ports],\n",
    "        'output_ports': [{'name': name, 'direction': 'output', 'width': width} for name, width in output_ports],\n",
    "        'signals': [{'name': name, 'type': s_type, 'width': width} for name, s_type, width in signals],\n",
    "        'parameters': [{'name': name, 'value': value} for name, value in parameters],\n",
    "        'operations': operations\n",
    "    }]\n",
    "\n",
    "    signal_dict = {}\n",
    "    for port in modules[0]['input_ports'] + modules[0]['output_ports'] + modules[0]['signals']:\n",
    "        signal_dict[port['name']] = {\n",
    "            'width': port['width'],\n",
    "            'type': port.get('type', 'wire' if port.get('direction') in ['input', 'output'] else port.get('type')),\n",
    "            'module': module_name,\n",
    "            'direction': port.get('direction', 'internal')\n",
    "        }\n",
    "\n",
    "    param_dict = {param['name']: {\n",
    "        'value': param['value'],\n",
    "        'module': module_name\n",
    "    } for param in modules[0]['parameters']}\n",
    "\n",
    "    operation_dict = {op['id']: {\n",
    "        'type': op['type'],\n",
    "        'target': op['target'],\n",
    "        'expression': op['expression'],\n",
    "        'operands': op['operands'],\n",
    "        'context': op['context'],\n",
    "        'module': module_name\n",
    "    } for op in operations}\n",
    "\n",
    "    relationships = []\n",
    "    for op in operations:\n",
    "        op_id = op['id']\n",
    "        for operand in op['operands']:\n",
    "            operand_clean = re.sub(r'\\[\\d+:\\d+\\]', '', operand)\n",
    "            if operand_clean in signal_dict:\n",
    "                relationships.append({\n",
    "                    'source': f\"operation_{urllib.parse.quote(op_id)}\",\n",
    "                    'target': f\"signal_{urllib.parse.quote(operand_clean)}\",\n",
    "                    'type': 'uses_signal'\n",
    "                })\n",
    "        target_clean = re.sub(r'\\[\\d+:\\d+\\]', '', op['target'])\n",
    "        if target_clean in signal_dict:\n",
    "            relationships.append({\n",
    "                'source': f\"operation_{urllib.parse.quote(op_id)}\",\n",
    "                'target': f\"signal_{urllib.parse.quote(target_clean)}\",\n",
    "                'type': 'produces_signal'\n",
    "            })\n",
    "        for param_name in param_dict:\n",
    "            if param_name in op['expression']:\n",
    "                relationships.append({\n",
    "                    'source': f\"operation_{urllib.parse.quote(op_id)}\",\n",
    "                    'target': f\"param_{urllib.parse.quote(param_name)}\",\n",
    "                    'type': 'depends_on_parameter'\n",
    "                })\n",
    "        if op['type'] == 'INSTANTIATION':\n",
    "            module_type = op['expression'].split('(')[0]\n",
    "            relationships.append({\n",
    "                'source': f\"module_{urllib.parse.quote(module_name)}\",\n",
    "                'target': f\"module_{urllib.parse.quote(module_type)}\",\n",
    "                'type': 'instantiates'\n",
    "            })\n",
    "\n",
    "    for signal_name, signal_info in signal_dict.items():\n",
    "        for param_name in param_dict:\n",
    "            if param_name in signal_info['width']:\n",
    "                relationships.append({\n",
    "                    'source': f\"signal_{urllib.parse.quote(signal_name)}\",\n",
    "                    'target': f\"param_{urllib.parse.quote(param_name)}\",\n",
    "                    'type': 'uses_parameter'\n",
    "                })\n",
    "\n",
    "    return modules, signal_dict, param_dict, operation_dict, relationships\n",
    "\n",
    "def create_knowledge_graph(modules, signals, parameters, operations, relationships, output_file):\n",
    "    g = Graph()\n",
    "    EX = Namespace('http://example.org/hw#')\n",
    "    g.bind('ex', EX)\n",
    "\n",
    "    g.add((EX.Module, RDF.type, RDFS.Class))\n",
    "    g.add((EX.Signal, RDF.type, RDFS.Class))\n",
    "    g.add((EX.Parameter, RDF.type, RDFS.Class))\n",
    "    g.add((EX.Operation, RDF.type, RDFS.Class))\n",
    "    g.add((EX.hasInput, RDF.type, RDF.Property))\n",
    "    g.add((EX.hasOutput, RDF.type, RDF.Property))\n",
    "    g.add((EX.hasInternalSignal, RDF.type, RDF.Property))\n",
    "    g.add((EX.hasParameter, RDF.type, RDF.Property))\n",
    "    g.add((EX.performsOperation, RDF.type, RDF.Property))\n",
    "    g.add((EX.hasExpression, RDF.type, RDF.Property))\n",
    "    g.add((EX.usesSignal, RDF.type, RDF.Property))\n",
    "    g.add((EX.producesSignal, RDF.type, RDF.Property))\n",
    "    g.add((EX.dependsOnParameter, RDF.type, RDF.Property))\n",
    "    g.add((EX.usesParameter, RDF.type, RDF.Property))\n",
    "    g.add((EX.instantiates, RDF.type, RDF.Property))\n",
    "\n",
    "    for module in modules:\n",
    "        module_uri = EX[f\"module_{urllib.parse.quote(module['name'])}\"]\n",
    "        g.add((module_uri, RDF.type, EX.Module))\n",
    "        g.add((module_uri, RDFS.label, Literal(module['name'])))\n",
    "\n",
    "        for port in module['input_ports']:\n",
    "            signal_uri = EX[f\"signal_{urllib.parse.quote(port['name'])}\"]\n",
    "            g.add((signal_uri, RDF.type, EX.Signal))\n",
    "            g.add((signal_uri, RDFS.label, Literal(port['name'])))\n",
    "            g.add((signal_uri, EX.width, Literal(port['width'])))\n",
    "            g.add((signal_uri, EX.direction, Literal('input')))\n",
    "            g.add((module_uri, EX.hasInput, signal_uri))\n",
    "\n",
    "        for port in module['output_ports']:\n",
    "            signal_uri = EX[f\"signal_{urllib.parse.quote(port['name'])}\"]\n",
    "            g.add((signal_uri, RDF.type, EX.Signal))\n",
    "            g.add((signal_uri, RDFS.label, Literal(port['name'])))\n",
    "            g.add((signal_uri, EX.width, Literal(port['width'])))\n",
    "            g.add((signal_uri, EX.direction, Literal('output')))\n",
    "            g.add((module_uri, EX.hasOutput, signal_uri))\n",
    "\n",
    "        for signal_name, signal_info in signals.items():\n",
    "            if signal_info['module'] == module['name'] and signal_info['direction'] == 'internal':\n",
    "                signal_uri = EX[f\"signal_{urllib.parse.quote(signal_name)}\"]\n",
    "                g.add((signal_uri, RDF.type, EX.Signal))\n",
    "                g.add((signal_uri, RDFS.label, Literal(signal_name)))\n",
    "                g.add((signal_uri, EX.width, Literal(signal_info['width'])))\n",
    "                g.add((signal_uri, EX.signalType, Literal(signal_info['type'])))\n",
    "                g.add((signal_uri, EX.direction, Literal('internal')))\n",
    "                g.add((module_uri, EX.hasInternalSignal, signal_uri))\n",
    "\n",
    "        for param_name, param_info in parameters.items():\n",
    "            if param_info['module'] == module['name']:\n",
    "                param_uri = EX[f\"param_{urllib.parse.quote(param_name)}\"]\n",
    "                g.add((param_uri, RDF.type, EX.Parameter))\n",
    "                g.add((param_uri, RDFS.label, Literal(param_name)))\n",
    "                g.add((param_uri, EX.value, Literal(param_info['value'])))\n",
    "                g.add((module_uri, EX.hasParameter, param_uri))\n",
    "\n",
    "        for op_id, op_info in operations.items():\n",
    "            if op_info['module'] == module['name']:\n",
    "                op_uri = EX[f\"operation_{urllib.parse.quote(op_id)}\"]\n",
    "                g.add((op_uri, RDF.type, EX.Operation))\n",
    "                g.add((op_uri, RDFS.label, Literal(op_info['type'])))\n",
    "                g.add((op_uri, EX.target, Literal(op_info['target'])))\n",
    "                g.add((op_uri, EX.hasExpression, Literal(op_info['expression'])))\n",
    "                g.add((op_uri, EX.context, Literal(op_info['context'])))\n",
    "                for operand in op_info['operands']:\n",
    "                    operand_clean = re.sub(r'\\[\\d+:\\d+\\]', '', operand)\n",
    "                    if operand_clean in signals:\n",
    "                        signal_uri = EX[f\"signal_{urllib.parse.quote(operand_clean)}\"]\n",
    "                        g.add((op_uri, EX.usesSignal, signal_uri))\n",
    "                g.add((module_uri, EX.performsOperation, op_uri))\n",
    "\n",
    "    for rel in relationships:\n",
    "        source_uri = EX[rel['source']]\n",
    "        target_uri = EX[rel['target']]\n",
    "        rel_type = EX[rel['type'].replace('_', '')]\n",
    "        g.add((source_uri, rel_type, target_uri))\n",
    "\n",
    "    g.serialize(destination=output_file, format='turtle')\n",
    "    print(f'Knowledge graph saved to {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a236b",
   "metadata": {},
   "source": [
    "## 7. CSV Processing & Embeddings\n",
    "**Expected CSV schema:**\n",
    "- If your data uses the Alpaca-style format, the `text` column may contain segments like `<s>[INST]...[/INST] ...</s>`.\n",
    "  - The instruction goes to `instruction` and the following Verilog block goes to `code`.\n",
    "- If your CSV contains standalone Verilog modules, the function detects `module ... endmodule` blocks directly.\n",
    "\n",
    "**Embeddings:**\n",
    "- For each chunk, build a text payload: *Instruction + Code + Summary*.\n",
    "- Create embeddings with `text-embedding-3-small` and write to ChromaDB with the full document and metadata.\n",
    "\n",
    "**Chroma metadata fields:** `id`, `instruction`, `summary`, `row_index`, `knowledge_graph` (path to TTL), `module_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(csv_file):\n",
    "    chunks = []\n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        print(f'CSV Headers: {reader.fieldnames}')\n",
    "        for row in reader:\n",
    "            if 'text' in row:\n",
    "                chunk = row['text']\n",
    "            elif 'code' in row:\n",
    "                chunk = row['code']\n",
    "            elif reader.fieldnames:\n",
    "                chunk = row[reader.fieldnames[0]]\n",
    "            else:\n",
    "                print(f'Row {reader.line_num}: No valid column found')\n",
    "                continue\n",
    "\n",
    "            inst_match = re.match(r'<s>\\[INST\\](.*?)\\[/INST\\](.*)</s>', chunk, re.DOTALL)\n",
    "            if inst_match:\n",
    "                instruction = inst_match.group(1).strip()\n",
    "                code = inst_match.group(2).strip()\n",
    "                chunks.append({\n",
    "                    'id': str(uuid.uuid4()),\n",
    "                    'instruction': instruction,\n",
    "                    'code': code,\n",
    "                    'row_index': reader.line_num - 1\n",
    "                })\n",
    "            else:\n",
    "                module_match = re.search(r'module\\s+(.*?)\\s+endmodule', chunk, re.DOTALL)\n",
    "                if module_match:\n",
    "                    code = module_match.group(0).strip()\n",
    "                    chunks.append({\n",
    "                        'id': str(uuid.uuid4()),\n",
    "                        'instruction': 'Process module code',\n",
    "                        'code': code,\n",
    "                        'row_index': reader.line_num - 1\n",
    "                    })\n",
    "                else:\n",
    "                    print(f'Invalid chunk format in row {reader.line_num}: {chunk[:50]}...')\n",
    "    return chunks\n",
    "\n",
    "def get_code_embedding(code):\n",
    "    try:\n",
    "        response = openai_client.embeddings.create(\n",
    "            input=code,\n",
    "            model='text-embedding-3-small'\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error generating embedding: {str(e)}')\n",
    "        return None\n",
    "\n",
    "def generate_code_embeddings(code_chunks):\n",
    "    embeddings = []\n",
    "    for chunk in code_chunks:\n",
    "        text = f\"Instruction: {chunk['instruction']}\\nCode:\\n{chunk['code']}\\nSummary:\\n{chunk['summary']}\"\n",
    "        embedding = get_code_embedding(text)\n",
    "        if embedding:\n",
    "            embeddings.append(embedding)\n",
    "        else:\n",
    "            embeddings.append([0] * 1536)\n",
    "            print(f\"Failed to generate embedding for chunk {chunk['id']}\")\n",
    "    return embeddings\n",
    "\n",
    "def store_in_chroma(chunks, embeddings, chroma_path, collection_name='verilog_modules'):\n",
    "    client_ch = chromadb.PersistentClient(path=chroma_path)\n",
    "    try:\n",
    "        client_ch.delete_collection(collection_name)\n",
    "    except:\n",
    "        pass\n",
    "    collection = client_ch.create_collection(collection_name)\n",
    "\n",
    "    valid_chunks = []\n",
    "    valid_embeddings = []\n",
    "    for chunk, emb in zip(chunks, embeddings):\n",
    "        if not all(x == 0 for x in emb):\n",
    "            valid_chunks.append(chunk)\n",
    "            valid_embeddings.append(emb)\n",
    "\n",
    "    if valid_chunks:\n",
    "        collection.add(\n",
    "            embeddings=[emb for emb in valid_embeddings],\n",
    "            documents=[f\"Instruction: {chunk['instruction']}\\nCode:\\n{chunk['code']}\\nSummary:\\n{chunk['summary']}\" for chunk in valid_chunks],\n",
    "            metadatas=[{\n",
    "                'id': chunk['id'],\n",
    "                'instruction': chunk['instruction'],\n",
    "                'summary': chunk['summary'],\n",
    "                'row_index': chunk['row_index'],\n",
    "                'knowledge_graph': chunk['knowledge_graph'],\n",
    "                'module_name': chunk.get('module_name', '')\n",
    "            } for chunk in valid_chunks],\n",
    "            ids=[chunk['id'] for chunk in valid_chunks]\n",
    "        )\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b4810",
   "metadata": {},
   "source": [
    "## 8. Build KG + Vector Store (End-to-End for Ingestion)\n",
    "**What happens in this section:**\n",
    "1. Parse each CSV row into chunks.\n",
    "2. Extract Verilog structure.\n",
    "3. Summarize with LLM (short textual description per module).\n",
    "4. Write RDF graphs to Turtle files.\n",
    "5. Compute embeddings and persist a Chroma collection.\n",
    "\n",
    "**Artifacts written:**\n",
    "- `chunk_metadata.json`: consolidated, per-chunk metadata for downstream retrieval.\n",
    "- `knowledge_graphs/kg_<id>.ttl`: RDF graph files.\n",
    "- `verilog_chroma_db/`: a persistent ChromaDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_kg(csv_file='extracted_texts1.csv'):\n",
    "    chroma_path = os.path.join(PERSIST_DIRECTORY, 'verilog_chroma_db')\n",
    "    metadata_file = os.path.join(PERSIST_DIRECTORY, 'chunk_metadata.json')\n",
    "\n",
    "    chunks = process_csv(csv_file)\n",
    "    print(f'Processed {len(chunks)} chunks from {csv_file}')\n",
    "\n",
    "    metadata = []\n",
    "    for chunk in chunks:\n",
    "        module_name, input_ports, output_ports, signals, parameters, operations, ast = parse_verilog_code(chunk['code'])\n",
    "        if module_name:\n",
    "            chunk['module_name'] = module_name\n",
    "            chunk['summary'] = summarize_code(chunk['code'], chunk['instruction'])\n",
    "            modules, signals_dict, param_dict, operation_dict, relationships = extract_entities(\n",
    "                module_name, input_ports, output_ports, signals, parameters, operations, ast\n",
    "            )\n",
    "            kg_file = os.path.join(PERSIST_DIRECTORY, 'knowledge_graphs', f\"kg_{chunk['id']}.ttl\")\n",
    "            create_knowledge_graph(modules, signals_dict, param_dict, operation_dict, relationships, kg_file)\n",
    "            chunk['knowledge_graph'] = kg_file\n",
    "            metadata.append({\n",
    "                'chunk_id': chunk['id'],\n",
    "                'module_name': module_name,\n",
    "                'knowledge_graph_path': kg_file,\n",
    "                'row_index': chunk['row_index'],\n",
    "                'instruction': chunk['instruction'],\n",
    "                'summary': chunk['summary'],\n",
    "                'code': chunk['code']\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Failed to parse module for chunk {chunk['id']}\")\n",
    "\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f'Metadata saved to {metadata_file}')\n",
    "\n",
    "    embeddings = generate_code_embeddings(chunks)\n",
    "    print(f'Generated embeddings for {len(embeddings)} chunks')\n",
    "\n",
    "    collection = store_in_chroma(chunks, embeddings, chroma_path)\n",
    "    print(f'Chroma DB saved to {chroma_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e547904",
   "metadata": {},
   "source": [
    "## 9. RTL Generator (Vector Retrieval → Prompt → LLM)\n",
    "**Goal:** produce large, synthesizable Verilog based on the query. The prompt includes:\n",
    "- Retrieved context (documents and KG metadata) for signal/operation awareness.\n",
    "- Explicit requirements (e.g., pipelining, FSMs) matching the provided script.\n",
    "\n",
    "**Retrieval fields:**\n",
    "- `documents`: full text used for semantic similarity\n",
    "- `metadatas`: `module_name`, `knowledge_graph` path, `id`, and summary\n",
    "\n",
    "**Output:** printed RTL code block for manual review and copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "\n",
    "def count_tokens(text, model='gpt-4o'):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def load_chunks_and_chroma(metadata_file, chroma_path, collection_name='verilog_modules'):\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        chunks = json.load(f)\n",
    "    client = chromadb.PersistentClient(path=chroma_path)\n",
    "    collection = client.get_collection(collection_name)\n",
    "    return chunks, collection\n",
    "\n",
    "def get_code_embedding(code):\n",
    "    response = openai_client.embeddings.create(input=code, model='text-embedding-3-small')\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def query_vector_db(query_text, collection, n_results=10):\n",
    "    query_embedding = get_code_embedding(query_text)\n",
    "    return collection.query(query_embeddings=[query_embedding], n_results=n_results)\n",
    "\n",
    "def get_module_info(g, module_name):\n",
    "    uri = URIRef(f\"http://example.org/hw#module_{urllib.parse.quote(module_name)}\")\n",
    "    if not any(s == uri for s, _, _ in g):\n",
    "        return None\n",
    "    def run(q):\n",
    "        results = g.query(q)\n",
    "        return [{var: str(row[var]) for var in results.vars} for row in results]\n",
    "    qstr = lambda x: x % urllib.parse.quote(module_name)\n",
    "    return {\n",
    "        'inputs': run(qstr(\"PREFIX ex: <http://example.org/hw#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?signal ?label ?width WHERE { ex:module_%s ex:hasInput ?signal . ?signal rdfs:label ?label . OPTIONAL { ?signal ex:width ?width . } }\")),\n",
    "        'outputs': run(qstr(\"PREFIX ex: <http://example.org/hw#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?signal ?label ?width WHERE { ex:module_%s ex:hasOutput ?signal . ?signal rdfs:label ?label . OPTIONAL { ?signal ex:width ?width . } }\")),\n",
    "        'signals': run(qstr(\"PREFIX ex: <http://example.org/hw#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?signal ?label ?width ?type WHERE { ex:module_%s ex:hasInternalSignal ?signal . ?signal rdfs:label ?label . OPTIONAL { ?signal ex:width ?width . } OPTIONAL { ?signal ex:signalType ?type . } }\"))[:5],\n",
    "        'parameters': run(qstr(\"PREFIX ex: <http://example.org/hw#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?param ?label ?value WHERE { ex:module_%s ex:hasParameter ?param . ?param rdfs:label ?label . ?param ex:value ?value . }\")),\n",
    "        'operations': run(qstr(\"PREFIX ex: <http://example.org/hw#> PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> SELECT ?operation ?label ?target ?expression WHERE { ex:module_%s ex:performsOperation ?operation . ?operation rdfs:label ?label . ?operation ex:target ?target . ?operation ex:hasExpression ?expression . }\"))[:5]\n",
    "    }\n",
    "\n",
    "def construct_llm_prompt(query, results, chunks):\n",
    "    prompt_template = \"\"\"\n",
    "# Verilog Module Generation\n",
    "**Query**: {query}\n",
    "\n",
    "**Context**: {module_details}\n",
    "\n",
    "**Task**:\n",
    "Generate a **full synthesizable Verilog RTL module** with **at least 500 lines**. You MUST:\n",
    "- Use deep pipelining with at least 3 pipeline stages for every major block.\n",
    "- Include multiple FSMs with at least 5 states each, documented inline.\n",
    "- Expand register files to multiple arrays with hundreds of signals.\n",
    "- Unroll loops for RAM initialization and complex data paths.\n",
    "- Add extra case statements and conditionals to reach line count.\n",
    "- Repeat non-critical always blocks with slight variations to pad size.\n",
    "- Use meaningful signal names and thorough comments for each section.\n",
    "Output only a single code block: `Verilog Module 500+ Lines`.\n",
    "\"\"\"\n",
    "    module_details = ''\n",
    "    for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "        chunk = next((c for c in chunks if c['chunk_id'] == meta['id']), None)\n",
    "        if not chunk:\n",
    "            continue\n",
    "        print(f\"Processing module: {meta['module_name']} | Chunk ID: {meta['id']} | KG: {meta['knowledge_graph']}\")\n",
    "        g = Graph()\n",
    "        g.parse(meta['knowledge_graph'], format='turtle')\n",
    "        module_info = get_module_info(g, meta['module_name'])\n",
    "        module_details += f\"Module: {meta['module_name']}\\nInstruction: {chunk.get('instruction','')}\\nSummary: {chunk.get('summary','')}\\nMetadata: {json.dumps(module_info, indent=2)}\\n\"\n",
    "    prompt = prompt_template.format(query=query, module_details=module_details)\n",
    "    return prompt\n",
    "\n",
    "def call_llm(prompt):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        max_tokens=16000\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_llm_rtl(query, chunks, collection):\n",
    "    results = query_vector_db(query, collection)\n",
    "    if results:\n",
    "        prompt = construct_llm_prompt(query, results, chunks)\n",
    "        output = call_llm(prompt)\n",
    "        print('\\nGenerated Large RTL Verilog:\\n', output if output else 'No response.')\n",
    "    else:\n",
    "        print('No DB results.')\n",
    "\n",
    "def main_rtl():\n",
    "    chunks, collection = load_chunks_and_chroma(\n",
    "        os.path.join(PERSIST_DIRECTORY, 'chunk_metadata.json'),\n",
    "        os.path.join(PERSIST_DIRECTORY, 'verilog_chroma_db'))\n",
    "    if not chunks or not collection:\n",
    "        print('Load failed.')\n",
    "        return\n",
    "    print(f'Loaded {len(chunks)} chunks.')\n",
    "    while True:\n",
    "        q = input(\"Query (or 'exit'): \")\n",
    "        if q.strip().lower() == 'exit':\n",
    "            break\n",
    "        query_llm_rtl(q, chunks, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877020de",
   "metadata": {},
   "source": [
    "## 10. SVA Generator (Vector Retrieval → Prompt → LLM)\n",
    "**Goal:** generate a standalone, synthesizable Verilog module and embed **SystemVerilog immediate assertions**\n",
    "that check operation correctness, bounds, input validity, and signal relations.\n",
    "\n",
    "**Prompt contents:** retrieved context (documents + KG facts), a compact code example, and precise requirements.\n",
    "\n",
    "**Output:** printed Verilog with assertions for manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa9852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_and_chroma_sva(metadata_file, chroma_path, collection_name='verilog_modules'):\n",
    "    try:\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "        client = chromadb.PersistentClient(path=chroma_path)\n",
    "        collection = client.get_collection(collection_name)\n",
    "        return chunks, collection\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load chunks or Chroma collection: {e}')\n",
    "        return None, None\n",
    "\n",
    "def get_text_embedding_sva(code):\n",
    "    try:\n",
    "        response = openai_client.embeddings.create(\n",
    "            input=code,\n",
    "            model='text-embedding-3-small'\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error generating embedding: {str(e)}')\n",
    "        return None\n",
    "\n",
    "def query_vector_db_sva(query_text, collection, n_results=1):\n",
    "    try:\n",
    "        query_embedding = get_text_embedding_sva(query_text)\n",
    "        if query_embedding:\n",
    "            results = collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            return results\n",
    "        else:\n",
    "            print('Failed to generate query embedding')\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f'Query error: {e}')\n",
    "        return None\n",
    "\n",
    "def get_module_info_sva(g, module_name):\n",
    "    module_uri = URIRef(f\"http://example.org/hw#module_{urllib.parse.quote(module_name)}\")\n",
    "    if not any(s == module_uri for s, _, _ in g):\n",
    "        print(f'No triples found for module {module_name}')\n",
    "        return None\n",
    "\n",
    "    input_query = \"\"\"\n",
    "    PREFIX ex: <http://example.org/hw#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?signal ?label ?width\n",
    "    WHERE {\n",
    "        ex:module_%s ex:hasInput ?signal .\n",
    "        ?signal rdfs:label ?label .\n",
    "        OPTIONAL { ?signal ex:width ?width . }\n",
    "    }\n",
    "    \"\"\" % urllib.parse.quote(module_name)\n",
    "\n",
    "    output_query = \"\"\"\n",
    "    PREFIX ex: <http://example.org/hw#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?signal ?label ?width\n",
    "    WHERE {\n",
    "        ex:module_%s ex:hasOutput ?signal .\n",
    "        ?signal rdfs:label ?label .\n",
    "        OPTIONAL { ?signal ex:width ?width . }\n",
    "    }\n",
    "    \"\"\" % urllib.parse.quote(module_name)\n",
    "\n",
    "    signal_query = \"\"\"\n",
    "    PREFIX ex: <http://example.org/hw#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?signal ?label ?width ?type\n",
    "    WHERE {\n",
    "        ex:module_%s ex:hasInternalSignal ?signal .\n",
    "        ?signal rdfs:label ?label .\n",
    "        OPTIONAL { ?signal ex:width ?width . }\n",
    "        OPTIONAL { ?signal ex:signalType ?type . }\n",
    "    }\n",
    "    \"\"\" % urllib.parse.quote(module_name)\n",
    "\n",
    "    parameter_query = \"\"\"\n",
    "    PREFIX ex: <http://example.org/hw#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?param ?label ?value\n",
    "    WHERE {\n",
    "        ex:module_%s ex:hasParameter ?param .\n",
    "        ?param rdfs:label ?label .\n",
    "        ?param ex:value ?value .\n",
    "    }\n",
    "    \"\"\" % urllib.parse.quote(module_name)\n",
    "\n",
    "    operation_query = \"\"\"\n",
    "    PREFIX ex: <http://example.org/hw#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?operation ?label ?target ?expression\n",
    "    WHERE {\n",
    "        ex:module_%s ex:performsOperation ?operation .\n",
    "        ?operation rdfs:label ?label .\n",
    "        ?operation ex:target ?target .\n",
    "        ?operation ex:hasExpression ?expression .\n",
    "    }\n",
    "    \"\"\" % urllib.parse.quote(module_name)\n",
    "\n",
    "    def execute_query(query):\n",
    "        results = g.query(query)\n",
    "        output = []\n",
    "        for row in results:\n",
    "            row_dict = {}\n",
    "            for var in results.vars:\n",
    "                value = row[var]\n",
    "                row_dict[str(var)] = str(value) if value is not None else None\n",
    "            output.append(row_dict)\n",
    "        return output\n",
    "\n",
    "    try:\n",
    "        inputs = execute_query(input_query)\n",
    "        outputs = execute_query(output_query)\n",
    "        signals = execute_query(signal_query)[:5]\n",
    "        parameters = execute_query(parameter_query)\n",
    "        operations = execute_query(operation_query)[:5]\n",
    "    except Exception as e:\n",
    "        print(f'Query error: {e}')\n",
    "        return None\n",
    "\n",
    "    module_info = {\n",
    "        'module': module_name,\n",
    "        'inputs': [{'name': row['label'], 'width': row.get('width', 'unknown')} for row in inputs],\n",
    "        'outputs': [{'name': row['label'], 'width': row.get('width', 'unknown')} for row in outputs],\n",
    "        'signals': [{'name': row['label'], 'width': row.get('width', 'unknown'), 'type': row.get('type', 'unknown')} for row in signals],\n",
    "        'parameters': [{'name': row['label'], 'value': row['value']} for row in parameters],\n",
    "        'operations': [{'type': row['label'], 'target': row['target'], 'expression': row['expression']} for row in operations]\n",
    "    }\n",
    "    return module_info\n",
    "\n",
    "def construct_llm_prompt_sva(query, results, chunks):\n",
    "    prompt_template = \"\"\"\n",
    "# Verilog Module Generation with Comprehensive Immediate Assertions\n",
    "**Query**: {query}\n",
    "\n",
    "**Context**: Relevant Verilog module details to guide the design.\n",
    "\n",
    "{module_details}\n",
    "\n",
    "**Task**:\n",
    "- Generate a standalone, synthesizable Verilog module strictly adhering to the query.\n",
    "- Ensure 100% syntactically correct Verilog code, compatible with EDA tools.\n",
    "- Implement all logic directly, no external module instantiations.\n",
    "- For case statements, define operation codes as `localparam` constants with numeric literals and use these constants in case items.\n",
    "- Include concise comments for functionality, inputs, outputs, and logic.\n",
    "- Include comprehensive SystemVerilog immediate assertions (`assert`) covering correctness, bounds, validity, and relationships.\n",
    "- Output a single code block labeled `Verilog Module with Comprehensive Immediate Assertions`.\n",
    "\"\"\"\n",
    "\n",
    "    module_details = ''\n",
    "    max_code_len = 300\n",
    "    max_summary_len = 100\n",
    "    for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "        chunk_id = meta['id']\n",
    "        chunk = next((c for c in chunks if c['chunk_id'] == chunk_id), None)\n",
    "        if not chunk:\n",
    "            continue\n",
    "        kg_file = meta['knowledge_graph']\n",
    "        g = Graph()\n",
    "        try:\n",
    "            g.parse(kg_file, format='turtle')\n",
    "        except Exception as e:\n",
    "            print(f'Failed to parse {kg_file}: {e}')\n",
    "            continue\n",
    "        module_name = meta['module_name']\n",
    "        module_info = get_module_info_sva(g, module_name)\n",
    "        instruction = chunk.get('instruction', meta.get('instruction', 'Unknown'))\n",
    "        code = chunk.get('code', 'Code not available')\n",
    "        summary = chunk.get('summary', meta.get('summary', 'Summary not available'))\n",
    "        code = code[:max_code_len] + ('...' if len(code) > max_code_len else '')\n",
    "        summary = summary[:max_summary_len] + ('...' if len(summary) > max_summary_len else '')\n",
    "        module_details += f\"Module: {module_name}\\nInstruction: {instruction}\\nSummary: {summary}\\nCode:\\n```verilog\\n{code}\\n```\\n\"\n",
    "        if module_info:\n",
    "            module_details += (\n",
    "                f\"Metadata:\\n  Inputs: {json.dumps(module_info['inputs'], indent=2)}\\n\"\n",
    "                f\"  Outputs: {json.dumps(module_info['outputs'], indent=2)}\\n\"\n",
    "                f\"  Signals: {json.dumps(module_info['signals'], indent=2)}\\n\"\n",
    "                f\"  Parameters: {json.dumps(module_info['parameters'], indent=2)}\\n\"\n",
    "                f\"  Operations: {json.dumps(module_info['operations'], indent=2)}\\n\"\n",
    "            )\n",
    "        module_details += '\\n---\\n'\n",
    "    return prompt_template.format(query=query, module_details=module_details)\n",
    "\n",
    "def call_llm_sva(prompt):\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model='gpt-4',\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            max_tokens=6000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error calling LLM: {str(e)}')\n",
    "        return None\n",
    "\n",
    "def query_llm_sva(query, chunks, collection, n_results=1):\n",
    "    results = query_vector_db_sva(query, collection, n_results)\n",
    "    if results:\n",
    "        prompt = construct_llm_prompt_sva(query, results, chunks)\n",
    "        llm_response = call_llm_sva(prompt)\n",
    "        if llm_response:\n",
    "            print('\\nGenerated Verilog Module with Assertions:\\n')\n",
    "            print(llm_response)\n",
    "        else:\n",
    "            print('Failed to get LLM response')\n",
    "    else:\n",
    "        print('Failed to query vector database')\n",
    "\n",
    "def main_sva():\n",
    "    metadata_file = os.path.join(PERSIST_DIRECTORY, 'chunk_metadata.json')\n",
    "    chroma_path = os.path.join(PERSIST_DIRECTORY, 'verilog_chroma_db')\n",
    "    chunks, collection = load_chunks_and_chroma_sva(metadata_file, chroma_path)\n",
    "    if not chunks or not collection:\n",
    "        print('Failed to load chunks or Chroma collection')\n",
    "        return\n",
    "    print(f'Loaded {len(chunks)} chunks and Chroma collection')\n",
    "    while True:\n",
    "        q = input(\"Query (or 'exit'): \")\n",
    "        if q.strip().lower() == 'exit':\n",
    "            break\n",
    "        query_llm_sva(q, chunks, collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430cce9d",
   "metadata": {},
   "source": [
    "## 11. Interactive Entry (`while True` loop) & Usage Notes\n",
    "**How to use this entrypoint:**\n",
    "1. Run the cell.\n",
    "2. Enter a mode:\n",
    "   - `1`: Upload a CSV and build the KG + Vector store (one-time per dataset).\n",
    "   - `2`: Start the **RTL** interactive loop and issue queries.\n",
    "   - `3`: Start the **SVA** interactive loop and issue queries.\n",
    "\n",
    "**Troubleshooting tips:**\n",
    "- If `Load failed` appears in modes 2/3, ensure you completed mode 1 and that `chunk_metadata.json` and the Chroma collection exist.\n",
    "- If embeddings fail for a row, the code logs it and continues; those rows are skipped when adding to Chroma.\n",
    "- If Pyverilog cannot parse a module, the regex fallback tries to recover ports/signals.\n",
    "\n",
    "**Reproducibility:**\n",
    "- The LLM outputs are non-deterministic across runs. To reduce variance, you can set model parameters at call sites.\n",
    "- All intermediate artifacts (TTL, JSON, Chroma) are written under the persistent directory so you can re-use them in later sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('Select mode:')\n",
    "    print('  1) Build KG + Chroma from CSV (main_kg)')\n",
    "    print('  2) RTL generation (main_rtl)')\n",
    "    print('  3) SVA generation (main_sva)')\n",
    "    mode = input('Enter 1/2/3: ').strip()\n",
    "    if mode == '1':\n",
    "        uploaded = files.upload()\n",
    "        csv_file = list(uploaded.keys())[0]\n",
    "        main_kg(csv_file)\n",
    "    elif mode == '2':\n",
    "        main_rtl()\n",
    "    elif mode == '3':\n",
    "        main_sva()\n",
    "    else:\n",
    "        print('Invalid choice.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
